@Article{Flores1986,
  author   = {Benito E Flores},
  journal  = {Omega},
  title    = {A pragmatic view of accuracy measurement in forecasting},
  year     = {1986},
  issn     = {0305-0483},
  number   = {2},
  pages    = {93-98},
  volume   = {14},
  doi      = {https://doi.org/10.1016/0305-0483(86)90013-7},
  url      = {https://www.sciencedirect.com/science/article/pii/0305048386900137},
}

@Article{Lehmann1950,
  author    = {E. L. Lehmann},
  journal   = {The Annals of Mathematical Statistics},
  title     = {{Some Principles of the Theory of Testing Hypotheses}},
  year      = {1950},
  number    = {1},
  pages     = {1 -- 26},
  volume    = {21},
  doi       = {10.1214/aoms/1177729884},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aoms/1177729884},
}

@Book{Dunn2018,
  author    = {PK Dunn and GK Smyth},
  publisher = {Springer New York},
  title     = { Generalized Linear Models With Examples in R },
  year      = {2018},
}

@article{anscombeTransformationPoissonBinomial1948,
  title = {The {{Transformation}} of {{Poisson}}, {{Binomial}} and {{Negative-Binomial Data}}},
  author = {Anscombe, F. J.},
  year = {1948},
  journal = {Biometrika},
  volume = {35},
  number = {3/4},
  pages = {246--254},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332343}
}



@article{bolinLocalScaleInvariance2021,
  title = {Local Scale Invariance and Robustness of Proper Scoring Rules},
  author = {Bolin, David and Wallin, Jonas},
  year = {2023},
  month = feb,
  journal = {Statistical Science},
  volume = {38},
  number = {1},
  pages = {140--159},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/22-STS864},
  note = {DOI: 10.1214/22-STS864}, 
  abstract = {Averages of proper scoring rules are often used to rank probabilistic forecasts. In many cases, the individual terms in these averages are based on observations and forecasts from different distributions. We show that some of the most popular proper scoring rules, such as the continuous ranked probability score (CRPS), give more importance to observations with large uncertainty, which can lead to unintuitive rankings. To describe this issue, we define the concept of local scale invariance for scoring rules. A new class of generalized proper kernel scoring rules is derived and as a member of this class we propose the scaled CRPS (SCRPS). This new proper scoring rule is locally scale invariant and, therefore, works in the case of varying uncertainty. Like the CRPS, it is computationally available for output from ensemble forecasts, and does not require the ability to evaluate densities of forecasts. We further define robustness of scoring rules, show why this also can be an important concept for average scores unless one is specifically interested in extremes, and derive new proper scoring rules that are robust against outliers. The theoretical findings are illustrated in three different applications from spatial statistics, stochastic volatility models and regression for count data.},
  keywords = {forecast ranking,Model selection,probabilistic forecasting,spatial statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VN5JJ4X2/Bolin and Wallin - 2023 - Local scale invariance and robustness of proper sc.pdf}
}



@article{bracherNationalSubnationalShortterm2021,
  title={National and subnational short-term forecasting of {COVID}-19 in {Germany} and {Poland}, early 2021},
  author={Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and Goergen, Konstantin and Ketterer, Jakob L and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fiedler, Jochen and Fuhrmann, Jan and Funk, Sebastian and Gambin, Anna and Gogolewski, Krzysztof and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Leith{\"a}user, Neele and Li, Michael L. and Meinke, Jan H. and Miasojedow, B{\l}a{\.z}ej and Michaud, Isaac J. and Mohring, Jan and Nouvellet, Pierre and Nowosielski, J{\c{e}}drzej M. and Ozanski, Tomasz and Radwan, Maciej and Rakowski, Franciszek and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Gneiting, Tilmann and Schienle, Melanie},
  journal={{Communications Medicine}},
  year={2022},
  doi={10.1038/s43856-022-00191-8},
  note = {DOI: 10.1038/s43856-022-00191-8}
}



@misc{CdcepiFlusightforecastdata2022,
  title = {Cdcepi/{{Flusight-forecast-data}}},
  author = {CDC},
  year = {2022},
  month = nov,
  note = {Data repository, \url{https://github.com/cdcepi/Flusight-forecast-data}},
  howpublished = {CDC Epidemic Prediction Initiative}
}

@article{elliottForecastingEconomicsFinance2016,
  title = {Forecasting in {{Economics}} and {{Finance}}},
  author = {Elliott, Graham and Timmermann, Allan},
  year = {2016},
  journal = {Annual Review of Economics},
  volume = {8},
  number = {1},
  pages = {81--110},
  doi = {10.1146/annurev-economics-080315-015346},
  note = {DOI: 10.1146/annurev-economics-080315-015346},
  abstract = {Practices used to address economic forecasting problems have undergone substantial changes over recent years. We review how such changes have influenced the ways in which a range of forecasting questions are being addressed. We also discuss the promises and challenges arising from access to big data. Finally, we review empirical evidence and experience accumulated from the use of forecasting methods to a range of economic and financial variables.},
  keywords = {big data,forecast evaluation,forecast models,model instability,model misspecification,parameter estimation,risk},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-economics-080315-015346},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3AXCLA59/Elliott and Timmermann - 2016 - Forecasting in Economics and Finance.pdf}
}


@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid-19}} Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, Nikos I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and {Guyver-Fletcher}, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and {Perez-Guzman}, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and {van Leeuwen}, E. and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  year = {2020},
  month = nov,
  journal = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.11.11.20220962},
  note = {Preprint, \url{https://www.medrxiv.org/content/10.1101/2020.11.11.20220962v2}},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/Users/nikos/github-synced/zotero-nikos/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html}
}


@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  note = {DOI: 10.1111/j.1467-9868.2007.00587.x},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}


@article{kukkonenReviewOperationalRegionalscale2012,
  title = {A Review of Operational, Regional-Scale, Chemical Weather Forecasting Models in {{Europe}}},
  author = {Kukkonen, J. and Olsson, T. and Schultz, D. M. and Baklanov, A. and Klein, T. and Miranda, A. I. and Monteiro, A. and Hirtl, M. and Tarvainen, V. and Boy, M. and Peuch, V.-H. and Poupkou, A. and Kioutsioukis, I. and Finardi, S. and Sofiev, M. and Sokhi, R. and Lehtinen, K. E. J. and Karatzas, K. and San Jos{\'e}, R. and Astitha, M. and Kallos, G. and Schaap, M. and Reimer, E. and Jakobs, H. and Eben, K.},
  year = {2012},
  month = jan,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {1},
  pages = {1--87},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-12-1-2012},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Numerical models that combine weather forecasting and atmospheric chemistry are here referred to as chemical weather forecasting models. Eighteen operational chemical weather forecasting models on regional and continental scales in Europe are described and compared in this article. Topics discussed in this article include how weather forecasting and atmospheric chemistry models are integrated into chemical weather forecasting systems, how physical processes are incorporated into the models through parameterization schemes, how the model architecture affects the predicted variables, and how air chemistry and aerosol processes are formulated. In addition, we discuss sensitivity analysis and evaluation of the models, user operational requirements, such as model availability and documentation, and output availability and dissemination. In this manner, this article allows for the evaluation of the relative strengths and weaknesses of the various modelling systems and modelling approaches. Finally, this article highlights the most prominent gaps of knowledge for chemical weather forecasting models and suggests potential priorities for future research directions, for the following selected focus areas: emission inventories, the integration of numerical weather prediction and atmospheric chemical transport models, boundary conditions and nesting of models, data assimilation of the various chemical species, improved understanding and parameterization of physical processes, better evaluation of models against data and the construction of model ensembles.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X3N7D4HE/Kukkonen et al. - 2012 - A review of operational, regional-scale, chemical .pdf;/Users/nikos/github-synced/zotero-nikos/storage/XWR2S6F8/2012.html}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  note = {DOI: 10.1287/mnsc.22.10.1087},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}


@article{timmermannForecastingMethodsFinance2018a,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  month = nov,
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110217-022713},
  note = {DOI: 10.1146/annurev-financial-110217-022713},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FHI5V4F/Timmermann - 2018 - Forecasting Methods in Finance.pdf}
}


@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  note = {DOI: 10.1007/BF02562681},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}

@article{yuVarianceStabilizingTransformations2009,
  title = {Variance Stabilizing Transformations of {{Poisson}}, Binomial and Negative Binomial Distributions},
  author = {Yu, Guan},
  year = {2009},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {79},
  number = {14},
  pages = {1621--1629},
  issn = {01677152},
  doi = {10.1016/j.spl.2009.04.010},
  note = {DOI: 10.1016/j.spl.2009.04.010},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9WEQEI2D/Yu - 2009 - Variance stabilizing transformations of Poisson, b.pdf}
}
