@Article{Flores1986,
  author   = {Benito E Flores},
  journal  = {Omega},
  title    = {A pragmatic view of accuracy measurement in forecasting},
  year     = {1986},
  issn     = {0305-0483},
  number   = {2},
  pages    = {93-98},
  volume   = {14},
  doi      = {https://doi.org/10.1016/0305-0483(86)90013-7},
  url      = {https://www.sciencedirect.com/science/article/pii/0305048386900137},
}

@Article{Lehmann1950,
  author    = {E. L. Lehmann},
  journal   = {The Annals of Mathematical Statistics},
  title     = {{Some Principles of the Theory of Testing Hypotheses}},
  year      = {1950},
  number    = {1},
  pages     = {1 -- 26},
  volume    = {21},
  doi       = {10.1214/aoms/1177729884},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aoms/1177729884},
}

@Book{Dunn2018,
  author    = {PK Dunn and GK Smyth},
  publisher = {Springer New York},
  title     = { Generalized Linear Models With Examples in R },
  year      = {2018},
}

@article{anscombeTransformationPoissonBinomial1948,
  title = {The {{Transformation}} of {{Poisson}}, {{Binomial}} and {{Negative-Binomial Data}}},
  author = {Anscombe, F. J.},
  year = {1948},
  journal = {Biometrika},
  volume = {35},
  number = {3/4},
  pages = {246--254},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332343}
}



@article{bolinLocalScaleInvariance2021,
  title = {Local Scale Invariance and Robustness of Proper Scoring Rules},
  author = {Bolin, David and Wallin, Jonas},
  year = {2023},
  month = feb,
  journal = {Statistical Science},
  volume = {38},
  number = {1},
  pages = {140--159},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/22-STS864},
  note = {DOI: 10.1214/22-STS864}, 
  abstract = {Averages of proper scoring rules are often used to rank probabilistic forecasts. In many cases, the individual terms in these averages are based on observations and forecasts from different distributions. We show that some of the most popular proper scoring rules, such as the continuous ranked probability score (CRPS), give more importance to observations with large uncertainty, which can lead to unintuitive rankings. To describe this issue, we define the concept of local scale invariance for scoring rules. A new class of generalized proper kernel scoring rules is derived and as a member of this class we propose the scaled CRPS (SCRPS). This new proper scoring rule is locally scale invariant and, therefore, works in the case of varying uncertainty. Like the CRPS, it is computationally available for output from ensemble forecasts, and does not require the ability to evaluate densities of forecasts. We further define robustness of scoring rules, show why this also can be an important concept for average scores unless one is specifically interested in extremes, and derive new proper scoring rules that are robust against outliers. The theoretical findings are illustrated in three different applications from spatial statistics, stochastic volatility models and regression for count data.},
  keywords = {forecast ranking,Model selection,probabilistic forecasting,spatial statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VN5JJ4X2/Bolin and Wallin - 2023 - Local scale invariance and robustness of proper sc.pdf}
}



@article{bracherNationalSubnationalShortterm2021,
  title={National and subnational short-term forecasting of {COVID}-19 in {Germany} and {Poland}, early 2021},
  author={Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and Goergen, Konstantin and Ketterer, Jakob L and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fiedler, Jochen and Fuhrmann, Jan and Funk, Sebastian and Gambin, Anna and Gogolewski, Krzysztof and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Leith{\"a}user, Neele and Li, Michael L. and Meinke, Jan H. and Miasojedow, B{\l}a{\.z}ej and Michaud, Isaac J. and Mohring, Jan and Nouvellet, Pierre and Nowosielski, J{\c{e}}drzej M. and Ozanski, Tomasz and Radwan, Maciej and Rakowski, Franciszek and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Gneiting, Tilmann and Schienle, Melanie},
  journal={{Communications Medicine}},
  year={2022},
  doi={10.1038/s43856-022-00191-8},
  note = {DOI: 10.1038/s43856-022-00191-8}
}



@misc{CdcepiFlusightforecastdata2022,
  title = {Cdcepi/{{Flusight-forecast-data}}},
  author = {CDC},
  year = {2022},
  month = nov,
  note = {Data repository, \url{https://github.com/cdcepi/Flusight-forecast-data}},
  howpublished = {CDC Epidemic Prediction Initiative}
}

@article{elliottForecastingEconomicsFinance2016,
  title = {Forecasting in {{Economics}} and {{Finance}}},
  author = {Elliott, Graham and Timmermann, Allan},
  year = {2016},
  journal = {Annual Review of Economics},
  volume = {8},
  number = {1},
  pages = {81--110},
  doi = {10.1146/annurev-economics-080315-015346},
  note = {DOI: 10.1146/annurev-economics-080315-015346},
  abstract = {Practices used to address economic forecasting problems have undergone substantial changes over recent years. We review how such changes have influenced the ways in which a range of forecasting questions are being addressed. We also discuss the promises and challenges arising from access to big data. Finally, we review empirical evidence and experience accumulated from the use of forecasting methods to a range of economic and financial variables.},
  keywords = {big data,forecast evaluation,forecast models,model instability,model misspecification,parameter estimation,risk},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-economics-080315-015346},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3AXCLA59/Elliott and Timmermann - 2016 - Forecasting in Economics and Finance.pdf}
}


@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid-19}} Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, Nikos I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and {Guyver-Fletcher}, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and {Perez-Guzman}, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and {van Leeuwen}, E. and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  year = {2020},
  month = nov,
  journal = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.11.11.20220962},
  note = {Preprint, \url{https://www.medrxiv.org/content/10.1101/2020.11.11.20220962v2}},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/Users/nikos/github-synced/zotero-nikos/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html}
}


@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  note = {DOI: 10.1111/j.1467-9868.2007.00587.x},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}


@article{kukkonenReviewOperationalRegionalscale2012,
  title = {A Review of Operational, Regional-Scale, Chemical Weather Forecasting Models in {{Europe}}},
  author = {Kukkonen, J. and Olsson, T. and Schultz, D. M. and Baklanov, A. and Klein, T. and Miranda, A. I. and Monteiro, A. and Hirtl, M. and Tarvainen, V. and Boy, M. and Peuch, V.-H. and Poupkou, A. and Kioutsioukis, I. and Finardi, S. and Sofiev, M. and Sokhi, R. and Lehtinen, K. E. J. and Karatzas, K. and San Jos{\'e}, R. and Astitha, M. and Kallos, G. and Schaap, M. and Reimer, E. and Jakobs, H. and Eben, K.},
  year = {2012},
  month = jan,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {1},
  pages = {1--87},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-12-1-2012},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Numerical models that combine weather forecasting and atmospheric chemistry are here referred to as chemical weather forecasting models. Eighteen operational chemical weather forecasting models on regional and continental scales in Europe are described and compared in this article. Topics discussed in this article include how weather forecasting and atmospheric chemistry models are integrated into chemical weather forecasting systems, how physical processes are incorporated into the models through parameterization schemes, how the model architecture affects the predicted variables, and how air chemistry and aerosol processes are formulated. In addition, we discuss sensitivity analysis and evaluation of the models, user operational requirements, such as model availability and documentation, and output availability and dissemination. In this manner, this article allows for the evaluation of the relative strengths and weaknesses of the various modelling systems and modelling approaches. Finally, this article highlights the most prominent gaps of knowledge for chemical weather forecasting models and suggests potential priorities for future research directions, for the following selected focus areas: emission inventories, the integration of numerical weather prediction and atmospheric chemical transport models, boundary conditions and nesting of models, data assimilation of the various chemical species, improved understanding and parameterization of physical processes, better evaluation of models against data and the construction of model ensembles.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X3N7D4HE/Kukkonen et al. - 2012 - A review of operational, regional-scale, chemical .pdf;/Users/nikos/github-synced/zotero-nikos/storage/XWR2S6F8/2012.html}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  note = {DOI: 10.1287/mnsc.22.10.1087},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}


@article{timmermannForecastingMethodsFinance2018a,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  month = nov,
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110217-022713},
  note = {DOI: 10.1146/annurev-financial-110217-022713},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FHI5V4F/Timmermann - 2018 - Forecasting Methods in Finance.pdf}
}


@article{yuVarianceStabilizingTransformations2009,
  title = {Variance Stabilizing Transformations of {{Poisson}}, Binomial and Negative Binomial Distributions},
  author = {Yu, Guan},
  year = {2009},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {79},
  number = {14},
  pages = {1621--1629},
  issn = {01677152},
  doi = {10.1016/j.spl.2009.04.010},
  note = {DOI: 10.1016/j.spl.2009.04.010},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9WEQEI2D/Yu - 2009 - Variance stabilizing transformations of Poisson, b.pdf}
}


@misc{abbottEvaluatingEpidemiologicallyMotivated2022,
  title = {Evaluating an Epidemiologically Motivated Surrogate Model of a Multi-Model Ensemble},
  author = {Abbott, Sam and Sherratt, Katharine and Bosse, Nikos and Gruson, Hugo and Bracher, Johannes and Funk, Sebastian},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280917},
  publisher = {{medRxiv}},
  doi = {10.1101/2022.10.12.22280917},
  urldate = {2023-01-15},
  abstract = {Multi-model and multi-team ensemble forecasts have become widely used to generate reliable short-term predictions of infectious disease spread. Notably, various public health agencies have used them to leverage academic disease modelling during the COVID-19 pandemic. However, ensemble forecasts are difficult to interpret and require extensive effort from numerous participating groups as well as a coordination team. In other fields, resource usage has been reduced by training simplified models that reproduce some of the observed behaviour of more complex models. Here we used observations of the behaviour of the European COVID-19 Forecast Hub ensemble combined with our own forecasting experience to identify a set of properties present in current ensemble forecasts. We then developed a parsimonious forecast model intending to mirror these properties. We assess forecasts generated from this model in real time over six months (the 15th of January 2022 to the 19th of July 2022) and for multiple European countries. We focused on forecasts of cases one to four weeks ahead and compared them to those by the European forecast hub ensemble. We find that the surrogate model behaves qualitatively similarly to the ensemble in many instances, though with increased uncertainty and poorer performance around periods of peak incidence (as measured by the Weighted Interval Score). The performance differences, however, seem to be partially due to a subset of time points, and the proposed model appears better probabilistically calibrated than the ensemble. We conclude that our simplified forecast model may have captured some of the dynamics of the hub ensemble, but more work is needed to understand the implicit epidemiological model that it represents.},
  archiveprefix = {medRxiv},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {note: Preprint, \textbackslash url\{https://www.medrxiv.org/content/10.1101/2022.10.12.22280917v1\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/63KP8PMA/Abbott et al. - 2022 - Evaluating an epidemiologically motivated surrogat.pdf}
}

@article{anscombeTransformationPoissonBinomial1948,
  title = {The {{Transformation}} of {{Poisson}}, {{Binomial}} and {{Negative-Binomial Data}}},
  author = {Anscombe, F. J.},
  year = {1948},
  journal = {Biometrika},
  volume = {35},
  number = {3/4},
  eprint = {2332343},
  eprinttype = {jstor},
  pages = {246--254},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332343},
  urldate = {2022-12-07}
}

@article{bartlettSquareRootTransformation1936,
  title = {The {{Square Root Transformation}} in {{Analysis}} of {{Variance}}},
  author = {Bartlett, M. S.},
  year = {1936},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {3},
  number = {1},
  eprint = {2983678},
  eprinttype = {jstor},
  pages = {68--78},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {1466-6162},
  doi = {10.2307/2983678},
  urldate = {2022-12-07}
}

@misc{bellegoDealingLogsZeros2022,
  title = {Dealing with {{Logs}} and {{Zeros}} in {{Regression Models}}},
  author = {Bell{\'e}go, Christophe and Benatia, David and Pape, Louis},
  year = {2022},
  month = mar,
  number = {arXiv:2203.11820},
  eprint = {2203.11820},
  publisher = {{arXiv}},
  urldate = {2023-01-12},
  abstract = {Log-linear models are prevalent in empirical research. Yet, how to handle zeros in the dependent variable remains an unsettled issue. This article clarifies it and addresses the log of zero by developing a new family of estimators called iterated Ordinary Least Squares (iOLS). This family nests standard approaches such as log-linear and Poisson regressions, offers several computational advantages, and corresponds to the correct way to perform the popular \$\textbackslash log(Y+1)\$ transformation. We extend it to the endogenous regressor setting (i2SLS) and overcome other common issues with Poisson models, such as controlling for many fixed-effects. We also develop specification tests to help researchers select between alternative estimators. Finally, our methods are illustrated through numerical simulations and replications of landmark publications.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  annotation = {note: Preprint, \textbackslash url\{https://arxiv.org/abs/2203.11820\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C7KFPQU8/Bell√©go et al. - 2022 - Dealing with Logs and Zeros in Regression Models.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UC4DFC6L/2203.html}
}

@article{bolinLocalScaleInvariance2023,
  title = {Local Scale Invariance and Robustness of Proper Scoring Rules},
  author = {Bolin, David and Wallin, Jonas},
  year = {2023},
  month = feb,
  journal = {Statistical Science},
  volume = {38},
  number = {1},
  pages = {140--159},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/22-STS864},
  urldate = {2023-01-24},
  abstract = {Averages of proper scoring rules are often used to rank probabilistic forecasts. In many cases, the individual terms in these averages are based on observations and forecasts from different distributions. We show that some of the most popular proper scoring rules, such as the continuous ranked probability score (CRPS), give more importance to observations with large uncertainty, which can lead to unintuitive rankings. To describe this issue, we define the concept of local scale invariance for scoring rules. A new class of generalized proper kernel scoring rules is derived and as a member of this class we propose the scaled CRPS (SCRPS). This new proper scoring rule is locally scale invariant and, therefore, works in the case of varying uncertainty. Like the CRPS, it is computationally available for output from ensemble forecasts, and does not require the ability to evaluate densities of forecasts. We further define robustness of scoring rules, show why this also can be an important concept for average scores unless one is specifically interested in extremes, and derive new proper scoring rules that are robust against outliers. The theoretical findings are illustrated in three different applications from spatial statistics, stochastic volatility models and regression for count data.},
  keywords = {forecast ranking,Model selection,probabilistic forecasting,spatial statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VN5JJ4X2/Bolin and Wallin - 2023 - Local scale invariance and robustness of proper sc.pdf}
}

@article{bosseEvaluatingForecastsScoringutils2022,
  title = {Evaluating {{Forecasts}} with Scoringutils in {{R}}},
  author = {Bosse, Nikos I. and Gruson, Hugo and Cori, Anne and {van Leeuwen}, Edwin and Funk, Sebastian and Abbott, Sam},
  year = {2022},
  month = may,
  journal = {arXiv},
  eprint = {2205.07090},
  primaryclass = {stat},
  doi = {10.48550/arXiv.2205.07090},
  urldate = {2023-01-15},
  abstract = {Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  annotation = {Preprint, \textbackslash url\{https://arxiv.org/abs/2205.07090\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QW8YLYQZ/Bosse et al. - 2022 - Evaluating Forecasts with scoringutils in R.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DNAA9E73/2205.html}
}

@article{boxAnalysisTransformations1964,
  title = {An {{Analysis}} of {{Transformations}}},
  author = {Box, G. E. P. and Cox, D. R.},
  year = {1964},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {26},
  number = {2},
  eprint = {2984418},
  eprinttype = {jstor},
  pages = {211--252},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-01-09},
  abstract = {In the analysis of data it is often assumed that observations y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n} are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters \texttheta. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SNB9W2HD/Box and Cox - 1964 - An Analysis of Transformations.pdf}
}

@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  annotation = {note: DOI 10.1371/journal.pcbi.1008618},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{bracherNationalSubnationalShortterm2022,
  title = {National and Subnational Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during Early 2021},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and G{\"o}rgen, Konstantin and Ketterer, Jakob L. and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V. and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I. and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fiedler, Jochen and Fuhrmann, Jan and Funk, Sebastian and Gambin, Anna and Gogolewski, Krzysztof and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Leith{\"a}user, Neele and Li, Michael L. and Meinke, Jan H. and Miasojedow, B{\l}a{\.z}ej and Michaud, Isaac J. and Mohring, Jan and Nouvellet, Pierre and Nowosielski, Jedrzej M. and Ozanski, Tomasz and Radwan, Maciej and Rakowski, Franciszek and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Gneiting, Tilmann and Schienle, Melanie},
  year = {2022},
  month = oct,
  journal = {Communications Medicine},
  volume = {2},
  number = {1},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {2730-664X},
  doi = {10.1038/s43856-022-00191-8},
  urldate = {2023-02-22},
  abstract = {During the COVID-19 pandemic there has been a strong interest in forecasts of the short-term development of epidemiological indicators to inform decision makers. In this study we evaluate probabilistic real-time predictions of confirmed cases and deaths from COVID-19 in Germany and Poland for the period from January through April 2021.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Viral infection},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N3WD8QGU/Bracher et al. - 2022 - National and subnational short-term forecasting of.pdf}
}

@article{bracherShorttermForecastingCOVID192021,
  title = {Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave \textendash{} a Preregistered Study},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, Nikos I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = jan,
  journal = {medRxiv},
  pages = {2020.12.24.20248826},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.12.24.20248826},
  urldate = {2021-04-01},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We report insights from ten weeks of collaborative COVID-19 forecasting for Germany and Poland (12 October \textendash{} 19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BHPBLCD9/Bracher et al. - 2021 - Short-term forecasting of COVID-19 in Germany and .pdf;/Users/nikos/github-synced/zotero-nikos/storage/I3ULULUZ/2020.12.24.20248826v2.html}
}

@misc{cdcCdcepiFlusightforecastdata2022,
  title = {Cdcepi/{{Flusight-forecast-data}}},
  author = {{CDC}},
  year = {2022},
  month = nov,
  urldate = {2022-11-17},
  howpublished = {CDC Epidemic Prediction Initiative}
}

@misc{cramerCOVID19ForecastHub2020,
  title = {{{COVID-19 Forecast Hub}}: 4 {{December}} 2020 Snapshot},
  shorttitle = {{{COVID-19 Forecast Hub}}},
  author = {Cramer, Estee and Nicholas G Reich and Serena Yijin Wang and Jarad Niemi and Abdul Hannan and Katie House and Youyang Gu and Shanghong Xie and Steve Horstman and {aniruddhadiga} and Robert Walraven and {starkari} and Michael Lingzhi Li and Graham Gibson and Lauren Castro and Dean Karlen and Nutcha Wattanachit and {jinghuichen} and {zyt9lsb} and {aagarwal1996} and Spencer Woody and Evan Ray and Frost Tianjian Xu and Hannah Biegel and GuidoEspana and Xinyue X and Johannes Bracher and Elizabeth Lee and {har96} and {leyouz}},
  year = {2020},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3963371},
  urldate = {2021-05-29},
  abstract = {This update to the COVID-19 Forecast Hub repository is a snapshot as of 4 December 2020 of the data hosted by and visualized at~https://covid19forecasthub.org/.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AVWA2UPE/4305938.html}
}

@article{cramerEvaluationIndividualEnsemble2021,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{US}}},
  author = {Cramer, Estee and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Rivadeneira, Alvaro J. Castro and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and Snyder, Timothy L. and Wilson, Davison D. and McConnell, Steve and Walraven, Robert and Shi, Yunfeng and Ban, Xuegang and Hong, Qi-Jun and Kong, Stanley and Turtle, James A. and {Ben-Nun}, Michal and Riley, Pete and Riley, Steven and Koyluoglu, Ugur and DesRoches, David and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Ozcan, Gokce and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Ferres, Juan Lavista and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and y Piontti, Ana Pastore and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Penna, Nicolas D. and Celi, Leo A. and Sundar, Saketh and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Kinsey, Matt and Obrecht, R. F. and Tallaksen, Katharine and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gaikedu, Emmanuela and Hay, Simon and Lim, Steve and Murray, Chris and Pigott, David and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewel, Joel and Meakin, Sophie R. and Munday, James D. and Sherratt, Katherine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Li, Michael L. and Bertsimas, Dimitris and Lami, Omar Skali and Soni, Saksham and Bouardi, Hamza Tazi and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Hu, Addison J. and Jahja, Maria and Narasimhan, Balasubramanian and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {medRxiv},
  pages = {2021.02.03.21250974},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.02.03.21250974},
  urldate = {2021-04-06},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. In 2020, the COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized hundreds of thousands of specific predictions from more than 50 different academic, industry, and independent research groups. This manuscript systematically evaluates 23 models that regularly submitted forecasts of reported weekly incident COVID-19 mortality counts in the US at the state and national level. One of these models was a multi-model ensemble that combined all available forecasts each week. The performance of individual models showed high variability across time, geospatial units, and forecast horizons. Half of the models evaluated showed better accuracy than a na\"ive baseline model. In combining the forecasts from all teams, the ensemble showed the best overall probabilistic accuracy of any model. Forecast accuracy degraded as models made predictions farther into the future, with probabilistic accuracy at a 20-week horizon more than 5 times worse than when predicting at a 1-week horizon. This project underscores the role that collaboration and active coordination between governmental public health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/W82X9ZN5/Cramer et al. - 2021 - Evaluation of individual and ensemble probabilisti.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7MC6LGTC/2021.02.03.21250974v1.html}
}

@article{diksLikelihoodbasedScoringRules2011,
  title = {Likelihood-Based Scoring Rules for Comparing Density Forecasts in Tails},
  author = {Diks, Cees and Panchenko, Valentyn and {van Dijk}, Dick},
  year = {2011},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {163},
  number = {2},
  pages = {215--230},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2011.04.001},
  urldate = {2023-01-11},
  abstract = {We propose new scoring rules based on conditional and censored likelihood for assessing the predictive accuracy of competing density forecasts over a specific region of interest, such as the left tail in financial risk management. These scoring rules can be interpreted in terms of Kullback-Leibler divergence between weighted versions of the density forecast and the true density. Existing scoring rules based on weighted likelihood favor density forecasts with more probability mass in the given region, rendering predictive accuracy tests biased towards such densities. Using our novel likelihood-based scoring rules avoids this problem.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GCMRVXUV/Diks et al. - 2011 - Likelihood-based scoring rules for comparing densi.pdf}
}

@article{doi:10.2105/AJPH.2022.306831,
  title = {Collaborative Hubs: {{Making}} the Most of Predictive Epidemic Modeling},
  author = {Reich, Nicholas G. and Lessler, Justin and Funk, Sebastian and Viboud, Cecile and Vespignani, Alessandro and Tibshirani, Ryan J. and Shea, Katriona and Schienle, Melanie and Runge, Michael C. and Rosenfeld, Roni and Ray, Evan L. and Niehus, Rene and Johnson, Helen C. and Johansson, Michael A. and Hochheiser, Harry and Gardner, Lauren and Bracher, Johannes and Borchering, Rebecca K. and Biggerstaff, Matthew},
  year = {2022},
  journal = {American Journal of Public Health},
  volume = {112},
  number = {6},
  eprint = {https://doi.org/10.2105/AJPH.2022.306831},
  pages = {839--842},
  doi = {10.2105/AJPH.2022.306831}
}

@book{dunnGeneralizedLinearModels2018,
  title = {Generalized {{Linear Models With Examples}} in {{R}}},
  author = {Dunn, Peter K. and Smyth, Gordon K.},
  year = {2018},
  month = nov,
  publisher = {{Springer}},
  abstract = {This textbook presents an introduction to generalized linear models, complete with real-world data sets and practice problems, making it applicable for both beginning and advanced students of applied statistics. Generalized linear models (GLMs) are powerful tools in applied statistics that extend the ideas of multiple linear regression and analysis of variance to include response variables that are not normally distributed. As such, GLMs can model a wide variety of data types including counts, proportions, and binary outcomes or positive quantities.The book is designed with the student in mind, making it suitable for self-study or a structured course. Beginning with an introduction to linear regression, the book also devotes time to advanced topics not typically included in introductory textbooks. It features chapter introductions and summaries, clear examples, and many practice problems, all carefully designed to balance theory and practice. The text also provides a working knowledge of applied statistical practice through the extensive use of R, which is integrated into the text. Other features include: \textbullet{} Advanced topics such as power variance functions, saddlepoint approximations, likelihood score tests, modified profile likelihood, small-dispersion asymptotics, and randomized quantile residuals \textbullet{} Nearly 100 data sets in the companion R package GLMsData \textbullet{} Examples that are cross-referenced to the companion data set, allowing readers to load the data and follow the analysis in their own R session},
  googlebooks = {tBh5DwAAQBAJ},
  isbn = {978-1-4419-0118-7},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{dushoffSpeedStrengthEpidemic2021,
  title = {Speed and Strength of an Epidemic Intervention},
  author = {Dushoff, Jonathan and Park, Sang Woo},
  year = {2021},
  month = mar,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {288},
  number = {1947},
  pages = {20201556},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2020.1556},
  urldate = {2022-11-29},
  abstract = {An epidemic can be characterized by its strength (i.e., the reproductive number  {$\mathscr{R}$} R ) and speed (i.e., the exponential growth rate r). Disease modellers have historically placed much more emphasis on strength, in part because the effectiveness of an intervention strategy is typically evaluated on this scale. Here, we develop a mathematical framework for the classic, strength-based paradigm and show that there is a dual speed-based paradigm which can provide complementary insights. In particular, we note that r = 0 is a threshold for disease spread, just like  {$\mathscr{R}$}=1 R=1  [ 1], and show that we can measure the strength and speed of an intervention on the same scale as the strength and speed of an epidemic, respectively. We argue that, while the strength-based paradigm provides the clearest insight into certain questions, the speed-based paradigm provides the clearest view in other cases. As an example, we show that evaluating the prospects of `test-and-treat' interventions against the human immunodeficiency virus (HIV) can be done more clearly on the speed than strength scale, given uncertainty in the proportion of HIV spread that happens early in the course of infection. We also discuss evaluating the effects of the importance of pre-symptomatic transmission of the SARS-CoV-2 virus. We suggest that disease modellers should avoid over-emphasizing the reproductive number at the expense of the exponential growth rate, but instead look at these as complementary measures.},
  keywords = {exponential growth rate,mathematical epidemiology,reproductive number,speed and strength},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3VZUTIGT/Dushoff and Park - 2021 - Speed and strength of an epidemic intervention.pdf}
}

@article{elliottForecastingEconomicsFinance2016,
  title = {Forecasting in {{Economics}} and {{Finance}}},
  author = {Elliott, Graham and Timmermann, Allan},
  year = {2016},
  journal = {Annual Review of Economics},
  volume = {8},
  number = {1},
  pages = {81--110},
  doi = {10.1146/annurev-economics-080315-015346},
  urldate = {2021-12-15},
  abstract = {Practices used to address economic forecasting problems have undergone substantial changes over recent years. We review how such changes have influenced the ways in which a range of forecasting questions are being addressed. We also discuss the promises and challenges arising from access to big data. Finally, we review empirical evidence and experience accumulated from the use of forecasting methods to a range of economic and financial variables.},
  keywords = {big data,forecast evaluation,forecast models,model instability,model misspecification,parameter estimation,risk},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3AXCLA59/Elliott and Timmermann - 2016 - Forecasting in Economics and Finance.pdf}
}

@misc{europeancovid-19forecasthubEuropeanCovid19Forecast2021,
  title = {European {{Covid-19 Forecast Hub}}},
  author = {{European Covid-19 Forecast Hub}},
  year = {2021},
  urldate = {2021-05-30},
  howpublished = {https://covid19forecasthub.eu/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JRFUHRDI/covid19forecasthub.eu.html}
}

@article{fuglstadDoesNonstationarySpatial2015,
  title = {Does Non-Stationary Spatial Data Always Require Non-Stationary Random Fields?},
  author = {Fuglstad, Geir-Arne and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2015},
  month = nov,
  journal = {Spatial Statistics},
  volume = {14},
  pages = {505--531},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2015.10.001},
  urldate = {2022-12-07},
  abstract = {A stationary spatial model is an idealization and we expect that the true dependence structures of physical phenomena are spatially varying, but how should we handle this non-stationarity in practice? We study the challenges involved in applying a flexible non-stationary model to a dataset of annual precipitation in the conterminous US, where exploratory data analysis shows strong evidence of a non-stationary covariance structure. The aim of this paper is to investigate the modelling pipeline once non-stationarity has been detected in spatial data. We show that there is a real danger of over-fitting the model and that careful modelling is necessary in order to properly account for varying second-order structure. In fact, the example shows that sometimes non-stationary Gaussian random fields are not necessary to model non-stationary spatial data.},
  langid = {english},
  keywords = {Annual precipitation,Gaussian Markov random fields,Gaussian random fields,Non-stationary spatial modelling,Penalized maximum likelihood,Stochastic partial differential equations},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/F5ZMGL6I/Fuglstad et al. - 2015 - Does non-stationary spatial data always require no.pdf;/Users/nikos/github-synced/zotero-nikos/storage/RBWNKHT6/S2211675315000780.html}
}

@article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
  year = {2019},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {2},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  urldate = {2019-09-16},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013\textendash 16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  langid = {english},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JN28VVKF/article.html}
}

@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid-19}} Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, Nikos I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and {Guyver-Fletcher}, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and {Perez-Guzman}, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and {van Leeuwen}, E. and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  year = {2020},
  month = nov,
  journal = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.11.11.20220962},
  urldate = {2020-11-28},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/Users/nikos/github-synced/zotero-nikos/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html}
}

@article{gneitingMakingEvaluatingPoint2011a,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  year = {2011},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {494},
  pages = {746--762},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.r10138},
  urldate = {2022-11-30},
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  keywords = {Bayes rule,Bregman function,Conditional value-at-risk (CVaR),Decision theory,Elicitability,Expectile,Mean,Median,Mode,Proper scoring rule,Quantile,Statistical functional},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SGYXKRBX/Gneiting - 2011 - Making and Evaluating Point Forecasts.pdf}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  urldate = {2020-02-17},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  urldate = {2020-03-22},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{gneitingWeatherForecastingEnsemble2005,
  title = {Weather {{Forecasting}} with {{Ensemble Methods}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  year = {2005},
  month = oct,
  journal = {Science},
  volume = {310},
  number = {5746},
  pages = {248--249},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1115255},
  urldate = {2021-05-30},
  abstract = {{$<$}p{$>$} Traditional weather forecasting has been built on a foundation of deterministic modeling--start with initial conditions, put them into a supercomputer model, and end up with a prediction about future weather. But as Gneiting and Raftery discuss in their Perspective, a new approach--ensemble forecasting--was introduced in the early 1990s. In this method, up to 100 different computer runs, each with slightly different starting conditions or model assumptions, are combined into a weather forecast. In concert with statistical techniques, ensembles can provide accurate statements about the uncertainty in daily and seasonal forecasting. The challenge now is to improve the modeling, statistical analysis, and visualization technologies for disseminating the ensemble results. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2005 American Association for the Advancement of Science},
  langid = {english},
  pmid = {16224011},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VRJMN77J/Gneiting and Raftery - 2005 - Weather Forecasting with Ensemble Methods.pdf;/Users/nikos/github-synced/zotero-nikos/storage/8Q5UA2FU/248.html}
}

@article{goodRationalDecisions1952,
  title = {Rational {{Decisions}}},
  author = {Good, I. J.},
  year = {1952},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {14},
  number = {1},
  eprint = {2984087},
  eprinttype = {jstor},
  pages = {107--114},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2020-08-13},
  abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
  keywords = {Log Score,LogS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/23458422/2020 - Rational Decisions.pdf}
}

@article{gosticPracticalConsiderationsMeasuring2020,
  title = {Practical Considerations for Measuring the Effective Reproductive Number, {{Rt}}},
  author = {Gostic, Katelyn M. and McGough, Lauren and Baskerville, Ed and Abbott, Sam and Joshi, Keya and Tedijanto, Christine and Kahn, Rebecca and Niehus, Rene and Hay, James and {de Salazar}, Pablo and Hellewell, Joel and Meakin, Sophie and Munday, James and Bosse, Nikos I. and Sherrat, Katharine and Thompson, Robin N. and White, Laura F. and Huisman, Jana S. and Scire, J{\'e}r{\'e}mie and Bonhoeffer, Sebastian and Stadler, Tanja and Wallinga, Jacco and Funk, Sebastian and Lipsitch, Marc and Cobey, Sarah},
  year = {2020},
  month = jun,
  journal = {medRxiv},
  doi = {10.1101/2020.06.18.20134858},
  urldate = {2020-08-12},
  abstract = {Estimation of the effective reproductive number, Rt, is important for detecting changes in disease transmission over time. During the COVID-19 pandemic, policymakers and public health officials are using Rt to assess the effectiveness of interventions and to inform policy. However, estimation of Rt from available data presents several challenges, with critical implications for the interpretation of the course of the pandemic. The purpose of this document is to summarize these challenges, illustrate them with examples from synthetic data, and, where possible, make methodological recommendations. For near real-time estimation of Rt, we recommend the approach of Cori et al. (2013), which uses data from before time t and empirical estimates of the distribution of time between infections. Methods that require data from after time t, such as Wallinga and Teunis (2004), are conceptually and methodologically less suited for near real-time estimation, but may be appropriate for some retrospective analyses. We advise against using methods derived from Bettencourt and Ribeiro (2008), as the resulting Rt estimates may be biased if the underlying structural assumptions are not met. A challenge common to all approaches is reconstruction of the time series of new infections from observations occurring long after the moment of transmission. Naive approaches for dealing with observation delays, such as subtracting delays sampled from a distribution, can introduce bias. We provide suggestions for how to mitigate this and other technical challenges and highlight open problems in Rt estimation.},
  pmcid = {PMC7325187},
  pmid = {32607522},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/2DQ39CTT/Gostic et al. - 2020 - Practical considerations for measuring the effecti.pdf}
}

@article{guerreroTimeseriesAnalysisSupported1993,
  title = {Time-Series Analysis Supported by Power Transformations},
  author = {Guerrero, Victor M.},
  year = {1993},
  month = jan,
  journal = {Journal of Forecasting},
  volume = {12},
  number = {1},
  pages = {37--48},
  issn = {02776693, 1099131X},
  doi = {10.1002/for.3980120104},
  urldate = {2022-12-07},
  abstract = {This paper presents some procedures aimed at helping an applied timeseries analyst in the use of power transformations. Two methods are proposed for selecting a variance-stabilizing transformation and another for bias-reduction of the forecast in the original scale. Since these methods are essentially model-independent, they can be employed with practically any type of time-series model. Some comparisons are made with other methods currently available and it is shown that those proposed here are either easier to apply or are more general, with a performance similar to or better than other competing procedures.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MYBS9BRA/Guerrero - 1993 - Time-series analysis supported by power transforma.pdf}
}

@article{heldProbabilisticForecastingInfectious2017,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {22},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {age-structured contact matrix,endemic\textendash epidemic modelling,multivariate probabilistic forecasting,proper scoring rules,spatio-temporal surveillance data},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf}
}

@article{Johansson2019,
  title = {An Open Challenge to Advance Probabilistic Forecasting for Dengue Epidemics},
  author = {Johansson, Michael A. and Apfeldorf, Karyn M. and Dobson, Scott and Devita, Jason and Buczak, Anna L. and Baugher, Benjamin and Moniz, Linda J. and Bagley, Thomas and Babin, Steven M. and Guven, Erhan and Yamana, Teresa K. and Shaman, Jeffrey and Moschou, Terry and Lothian, Nick and Lane, Aaron and Osborne, Grant and Jiang, Gao and Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni and Lessler, Justin and Reich, Nicholas G. and Cummings, Derek A. T. and Lauer, Stephen A. and Moore, Sean M. and Clapham, Hannah E. and Lowe, Rachel and Bailey, Trevor C. and {Garc{\'i}a-D{\'i}ez}, Markel and Carvalho, Marilia S{\'a} and Rod{\'o}, Xavier and Sardar, Tridip and Paul, Richard and Ray, Evan L. and Sakrejda, Krzysztof and Brown, Alexandria C. and Meng, Xi and Osoba, Osonde and Vardavas, Raffaele and Manheim, David and Moore, Melinda and Rao, Dhananjai M. and Porco, Travis C. and Ackley, Sarah and Liu, Fengchen and Worden, Lee and Convertino, Matteo and Liu, Yang and Reddy, Abraham and Ortiz, Eloy and Rivero, Jorge and Brito, Humberto and Juarrero, Alicia and Johnson, Leah R. and Gramacy, Robert B. and Cohen, Jeremy M. and Mordecai, Erin A. and Murdock, Courtney C. and Rohr, Jason R. and Ryan, Sadie J. and {Stewart-Ibarra}, Anna M. and Weikel, Daniel P. and Jutla, Antarpreet and Khan, Rakibul and Poultney, Marissa and Colwell, Rita R. and {Rivera-Garc{\'i}a}, Brenda and Barker, Christopher M. and Bell, Jesse E. and Biggerstaff, Matthew and Swerdlow, David and {Mier-y-Teran-Romero}, Luis and Forshey, Brett M. and Trtanj, Juli and Asher, Jason and Clay, Matt and Margolis, Harold S. and Hebbeler, Andrew M. and George, Dylan and {Jean-Paul Chretien}},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {48},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1909865116},
  pages = {24268--24274},
  doi = {10.1073/pnas.1909865116}
}

@misc{kicimanCausalReasoningLarge2023,
  title = {Causal {{Reasoning}} and {{Large Language Models}}: {{Opening}} a {{New Frontier}} for {{Causality}}},
  shorttitle = {Causal {{Reasoning}} and {{Large Language Models}}},
  author = {K{\i}c{\i}man, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  year = {2023},
  month = may,
  number = {arXiv:2305.00050},
  eprint = {2305.00050},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-05},
  abstract = {The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain), and actual causality (86\% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7QXU2X5B/Kƒ±cƒ±man et al. - 2023 - Causal Reasoning and Large Language Models Openin.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ZFXAXZRM/2305.html}
}

@article{kukkonenReviewOperationalRegionalscale2012,
  title = {A Review of Operational, Regional-Scale, Chemical Weather Forecasting Models in {{Europe}}},
  author = {Kukkonen, J. and Olsson, T. and Schultz, D. M. and Baklanov, A. and Klein, T. and Miranda, A. I. and Monteiro, A. and Hirtl, M. and Tarvainen, V. and Boy, M. and Peuch, V.-H. and Poupkou, A. and Kioutsioukis, I. and Finardi, S. and Sofiev, M. and Sokhi, R. and Lehtinen, K. E. J. and Karatzas, K. and San Jos{\'e}, R. and Astitha, M. and Kallos, G. and Schaap, M. and Reimer, E. and Jakobs, H. and Eben, K.},
  year = {2012},
  month = jan,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {1},
  pages = {1--87},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-12-1-2012},
  urldate = {2021-12-15},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Numerical models that combine weather forecasting and atmospheric chemistry are here referred to as chemical weather forecasting models. Eighteen operational chemical weather forecasting models on regional and continental scales in Europe are described and compared in this article. Topics discussed in this article include how weather forecasting and atmospheric chemistry models are integrated into chemical weather forecasting systems, how physical processes are incorporated into the models through parameterization schemes, how the model architecture affects the predicted variables, and how air chemistry and aerosol processes are formulated. In addition, we discuss sensitivity analysis and evaluation of the models, user operational requirements, such as model availability and documentation, and output availability and dissemination. In this manner, this article allows for the evaluation of the relative strengths and weaknesses of the various modelling systems and modelling approaches. Finally, this article highlights the most prominent gaps of knowledge for chemical weather forecasting models and suggests potential priorities for future research directions, for the following selected focus areas: emission inventories, the integration of numerical weather prediction and atmospheric chemical transport models, boundary conditions and nesting of models, data assimilation of the various chemical species, improved understanding and parameterization of physical processes, better evaluation of models against data and the construction of model ensembles.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X3N7D4HE/Kukkonen et al. - 2012 - A review of operational, regional-scale, chemical .pdf;/Users/nikos/github-synced/zotero-nikos/storage/XWR2S6F8/2012.html}
}

@misc{lerchForecasterDilemmaExtreme2015,
  title = {Forecaster's {{Dilemma}}: {{Extreme Events}} and {{Forecast Evaluation}}},
  shorttitle = {Forecaster's {{Dilemma}}},
  author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
  year = {2015},
  month = dec,
  number = {arXiv:1512.09244},
  eprint = {1512.09244},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.09244},
  urldate = {2023-06-21},
  abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster's dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments, and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product growth, we illustrate and discuss the forecaster's dilemma along with potential remedies.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5KYJ2TJ8/Lerch et al. - 2015 - Forecaster's Dilemma Extreme Events and Forecast .pdf;/Users/nikos/github-synced/zotero-nikos/storage/MV8SFZSY/1512.html}
}

@article{loweStochasticRainfallrunoffForecasting2014,
  title = {Stochastic Rainfall-Runoff Forecasting: Parameter Estimation, Multi-Step Prediction, and Evaluation of Overflow Risk},
  shorttitle = {Stochastic Rainfall-Runoff Forecasting},
  author = {L{\"o}we, Roland and Mikkelsen, Peter Steen and Madsen, Henrik},
  year = {2014},
  month = mar,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {28},
  number = {3},
  pages = {505--516},
  issn = {1436-3259},
  doi = {10.1007/s00477-013-0768-0},
  urldate = {2022-12-07},
  abstract = {Probabilistic runoff forecasts generated by stochastic greybox models can be notably useful for the improvement of the decision-making process in real-time control setups for urban drainage systems because the prediction risk relationships in these systems are often highly nonlinear. To date, research has primarily focused on one-step-ahead flow predictions for identifying, estimating, and evaluating greybox models. For control purposes, however, stochastic predictions are required for longer forecast horizons and for the prediction of runoff volumes, rather than flows. This article therefore analyzes the quality of multistep ahead forecasts of runoff volume and considers new estimation methods based on scoring rules for k-step-ahead predictions. The study shows that the score-based methods are, in principle, suitable for the estimation of model parameters and can therefore help the identification of models for cases with noisy in-sewer observations. For the prediction of the overflow risk, no improvement was demonstrated through the application of stochastic forecasts instead of point predictions, although this result is thought to be caused by the notably simplified setup used in this analysis. In conclusion, further research must focus on the development of model structures that allow the proper separation of dry and wet weather uncertainties and simulate runoff uncertainties depending on the rainfall input.},
  langid = {english},
  keywords = {Multistep prediction,Online forecasting,Real-time control,Skill score,Stochastic greybox model,Urban drainage},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EK2FRDJY/L√∂we et al. - 2014 - Stochastic rainfall-runoff forecasting parameter .pdf}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  urldate = {2020-08-13},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}

@article{mayrLogLevelVAR2015,
  title = {Log versus Level in {{VAR}} Forecasting: 42 Million Empirical Answers\textemdash{{Expect}} the Unexpected},
  shorttitle = {Log versus Level in {{VAR}} Forecasting},
  author = {Mayr, Johannes and Ulbricht, Dirk},
  year = {2015},
  month = jan,
  journal = {Economics Letters},
  volume = {126},
  pages = {40--42},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2014.11.008},
  urldate = {2022-12-07},
  abstract = {The use of log-transformed data has become standard in macroeconomic forecasting with VAR models. However, its appropriateness in the context of out-of-sample forecasts has not yet been exposed to a thorough empirical investigation. With the aim of filling this void, a broad sample of VAR models is employed in a multi-country set up and approximately 42 million pseudo-out-of-sample forecasts of GDP are evaluated. The results show that, on average, the knee-jerk transformation of the data is at best harmless.},
  langid = {english},
  keywords = {Logarithmic transformation,Out-of-sample experiment,VAR-forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/UITFBGW3/Mayr and Ulbricht - 2015 - Log versus level in VAR forecasting 42 million em.pdf;/Users/nikos/github-synced/zotero-nikos/storage/IA4IHTS5/S0165176514004273.html}
}

@article{pellisChallengesControlCOVID192021,
  title = {Challenges in Control of {{COVID-19}}: Short Doubling Time and Long Delay to Effect of Interventions},
  shorttitle = {Challenges in Control of {{COVID-19}}},
  author = {Pellis, Lorenzo and Scarabel, Francesca and Stage, Helena B. and Overton, Christopher E. and Chappell, Lauren H. K. and Fearon, Elizabeth and Bennett, Emma and Lythgoe, Katrina A. and House, Thomas A. and Hall, Ian and {null}, null},
  year = {2021},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {376},
  number = {1829},
  pages = {20200264},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2020.0264},
  urldate = {2022-11-29},
  abstract = {Early assessments of the growth rate of COVID-19 were subject to significant uncertainty, as expected with limited data and difficulties in case ascertainment, but as cases were recorded in multiple countries, more robust inferences could be made. Using multiple countries, data streams and methods, we estimated that, when unconstrained, European COVID-19 confirmed cases doubled on average every 3 days (range 2.2\textendash 4.3 days) and Italian hospital and intensive care unit admissions every 2\textendash 3 days; values that are significantly lower than the 5\textendash 7 days dominating the early published literature. Furthermore, we showed that the impact of physical distancing interventions was typically not seen until at least 9 days after implementation, during which time confirmed cases could grow eightfold. We argue that such temporal patterns are more critical than precise estimates of the time-insensitive basic reproduction number R0 for initiating interventions, and that the combination of fast growth and long detection delays explains the struggle in countries' outbreak response better than large values of R0 alone. One year on from first reporting these results, reproduction numbers continue to dominate the media and public discourse, but robust estimates of unconstrained growth remain essential for planning worst-case scenarios, and detection delays are still key in informing the relaxation and re-implementation of interventions. This article is part of the theme issue `Modelling that shaped the early COVID-19 pandemic response in the UK'.},
  keywords = {early growth rate,incubation period,non-pharmaceutical interventions,onset-to-hospitalization delay,reproduction number,unconstrained epidemic},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FLM9YWNA/Pellis et al. - 2021 - Challenges in control of COVID-19 short doubling .pdf}
}

@article{reichCollaborativeMultiyearMultimodel2019,
  title = {A Collaborative Multiyear, Multimodel Assessment of Seasonal Influenza Forecasting in the {{United States}}},
  author = {Reich, Nicholas G. and Brooks, Logan C. and Fox, Spencer J. and Kandula, Sasikiran and McGowan, Craig J. and Moore, Evan and Osthus, Dave and Ray, Evan L. and Tushar, Abhinav and Yamana, Teresa K. and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
  year = {2019},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {8},
  pages = {3146--3154},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1812594116},
  urldate = {2021-10-13},
  abstract = {Influenza infects an estimated 9\textendash 35 million individuals each year in the United States and is a contributing cause for between 12,000 and 56,000 deaths annually. Seasonal outbreaks of influenza are common in temperate regions of the world, with highest incidence typically occurring in colder and drier months of the year. Real-time forecasts of influenza transmission can inform public health response to outbreaks. We present the results of a multiinstitution collaborative effort to standardize the collection and evaluation of forecasting models for influenza in the United States for the 2010/2011 through 2016/2017 influenza seasons. For these seven seasons, we assembled weekly real-time forecasts of seven targets of public health interest from 22 different models. We compared forecast accuracy of each model relative to a historical baseline seasonal average. Across all regions of the United States, over half of the models showed consistently better performance than the historical baseline when forecasting incidence of influenza-like illness 1 wk, 2 wk, and 3 wk ahead of available data and when forecasting the timing and magnitude of the seasonal peak. In some regions, delays in data reporting were strongly and negatively associated with forecast accuracy. More timely reporting and an improved overall accessibility to novel and traditional data sources are needed to improve forecasting accuracy and its integration with real-time public health decision making.},
  chapter = {PNAS Plus},
  copyright = {Copyright \textcopyright{} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {30647115},
  keywords = {forecasting,infectious disease,influenza,public health,statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XEKLR37W/Reich et al. - 2019 - A collaborative multiyear, multimodel assessment o.pdf;/Users/nikos/github-synced/zotero-nikos/storage/QID3PLW4/3146.html}
}

@article{sherrattPredictivePerformanceMultimodel2022,
  title = {Predictive Performance of Multi-Model Ensemble Forecasts of {{COVID-19}} across  {{European}} Nation},
  author = {Sherratt, K. and Gruson, H. and Grah, R. and Johnson, H. and Niehus, R. and Prasse, B. and Sandman, F. and Deuschel, J. and Wolffram, D. and Abbott, S. and Ullrich, A. and Gibson, G. and Ray, {\relax EL}. and Reich, {\relax NG}. and Sheldon, D. and Wang, Y. and Wattanachit, N. and Wang, L. and Trnka, J. and Obozinski, G. and Sun, T. and Thanou, D. and Pottier, L. and Krymova, E. and Barbarossa, {\relax MV}. and Leith{\"a}user, N. and Mohring, J. and Schneider, J. and Wlazlo, J. and Fuhrmann, J. and Lange, B. and Rodiah, I. and Baccam, P. and Gurung, H. and Stage, S. and Suchoski, B. and Budzinski, J. and Walraven, R. and Villanueva, I. and Tucek, V. and {\v S}m{\'i}d, M. and Zaj{\'i}cek, M. and P{\'e}rez, {\'A}lvarez C. and Reina, B. and Bosse, {\relax NI}. and Meakin, S. and Di Loro, Alaimo and Maruotti, A. and Eclerov{\'a}, V. and Kraus, A. and Kraus, D. and Pribylova, L. and Dimitris, B. and Li, {\relax ML}. and Saksham, S. and Dehning, J. and Mohr, S. and Priesemann, V. and Redlarski, G. and Bejar, B. and Ardenghi, G. and Parolini, N. and Ziarelli, G. and Bock, W. and Heyder, S. and Hotz, T. and E., Singh D. and {Guzman-Merino}, M. and Aznarte, {\relax JL}. and Mori{\~n}a, D. and Alonso, S. and {\'A}lvarez, E. and L{\'o}pez, D. and Prats, C. and Burgard, {\relax JP}. and Rodloff, A. and Zimmermann, T. and Kuhlmann, A. and Zibert, J. and Pennoni, F. and Divino, F. and Catal{\`a}, M. and Lovison, G. and Giudici, P. and Tarantino, B. and Bartolucci, F. and Jona, Lasinio G. and Mingione, M. and Farcomeni, A. and Srivastava, A. and {Montero-Manso}, P. and Adiga, A. and Hurt, B. and Lewis, B. and Marathe, M. and Porebski, P. and Venkatramanan, S. and Bartczuk, R. and Dreger, F. and Gambin, A. and Gogolewski, K. and {Gruziel-Slomka}, M. and Krupa, B. and Moszynski, A. and Niedzielewski, K. and Nowosielski, J. and Radwan, M. and Rakowski, F. and Semeniuk, M. and Szczurek, E. and Zielinski, J. and Kisielewski, J. and Pabjan, B. and Holger, K. and Kheifetz, Y. and Scholz, M. and Bodych, M. and Filinski, M. and Idzikowski, R. and Krueger, T. and Ozanski, T. and Bracher, J. and Funk, S.},
  year = {2022},
  journal = {Europe PMC},
  doi = {10.1101/2022.06.16.22276024},
  urldate = {2023-02-07},
  abstract = {Background  Short-term forecasts of infectious disease burden can contribute to  situational awareness and aid capacity planning. Based on best practice in  other fields and recent insights in infectious disease epidemiology, one can  maximise the predictive performance of such forecasts if multiple models are  combined into an ensemble. Here we report on the performance of ensembles in  predicting COVID-19 cases and deaths across Europe between 08 March 2021 and  07 March 2022. Methods  We used open-source tools to develop a public European COVID-19 Forecast  Hub. We invited groups globally to contribute weekly forecasts for COVID-19  cases and deaths reported from a standardised source over the next one to  four weeks. Teams submitted forecasts from March 2021 using standardised  quantiles of the predictive distribution. Each week we created an ensemble  forecast, where each predictive quantile was calculated as the  equally-weighted average (initially the mean and then from 26th July the  median) of all individual models' predictive quantiles. We measured the  performance of each model using the relative Weighted Interval Score (WIS),  comparing models' forecast accuracy relative to all other models. We  retrospectively explored alternative methods for ensemble forecasts,  including weighted averages based on models' past predictive  performance. Results  Over 52 weeks we collected and combined up to 28 forecast models for 32  countries. We found a weekly ensemble had a consistently strong performance  across countries over time. Across all horizons and locations, the ensemble  performed better on relative WIS than 84\% of participating models' forecasts  of incident cases (with a total N=862), and 92\% of participating models'  forecasts of deaths (N=746). Across a one to four week time horizon,  ensemble performance declined with longer forecast periods when forecasting  cases, but remained stable over four weeks for incident death forecasts. In  every forecast across 32 countries, the ensemble outperformed most  contributing models when forecasting either cases or deaths, frequently  outperforming all of its individual component models. Among several choices  of ensemble methods we found that the most influential and best choice was  to use a median average of models instead of using the mean, regardless of  methods of weighting component forecast models. Conclusions  Our results support the use of combining forecasts from individual  models into an ensemble in order to improve predictive performance across  epidemiological targets and populations during infectious disease epidemics.  Our findings further suggest that median ensemble methods yield better  predictive performance more than ones based on means. Our findings also  highlight that forecast consumers should place more weight on incident death  forecasts than incident case forecasts at forecast horizons greater than two  weeks. Code and data availability  All data and code are publicly available on Github:  covid19-forecast-hub-europe/euro-hub-ensemble.},
  copyright = {cc by},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HK3RSINT/Sherratt et al. - 2022 - Predictive performance of multi-model ensemble for.pdf}
}

@misc{srivastavaShapebasedEvaluationEpidemic2022,
  title = {Shape-Based {{Evaluation}} of {{Epidemic Forecasts}}},
  author = {Srivastava, Ajitesh and Singh, Satwant and Lee, Fiona},
  year = {2022},
  month = nov,
  number = {arXiv:2209.04035},
  eprint = {2209.04035},
  primaryclass = {q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.04035},
  urldate = {2022-11-21},
  abstract = {Infectious disease forecasting for ongoing epidemics has been traditionally performed, communicated, and evaluated as numerical targets - 1, 2, 3, and 4 week ahead cases, deaths, and hospitalizations. While there is great value in predicting these numerical targets to assess the burden of the disease, we argue that there is also value in communicating the future trend (description of the shape) of the epidemic -- for instance, if the cases will remain flat or a surge is expected. To ensure what is being communicated is useful we need to be able to evaluate how well the predicted shape matches with the ground truth shape. Instead of treating this as a classification problem (one out of \$n\$ shapes), we define a transformation of the numerical forecasts into a ``shapelet''-space representation. In this representation, each dimension corresponds to the similarity of the shape with one of the shapes of interest (a shapelet). We prove that this representation satisfies the property that two shapes that one would consider similar are mapped close to each other, and vice versa. We demonstrate that our representation is able to reasonably capture the trends in COVID-19 cases and deaths time-series. With this representation, we define an evaluation measure and a measure of agreement among multiple models. We also define the shapelet-space ensemble of multiple models as the mean of their shapelet-space representations. We show that this ensemble is able to accurately predict the shape of the future trend for COVID-19 cases and trends. We also show that the agreement between models can provide a good indicator of the reliability of the forecast.},
  archiveprefix = {arxiv},
  keywords = {Quantitative Biology - Quantitative Methods,Statistics - Applications},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NDNZDX3Z/Srivastava et al. - 2022 - Shape-based Evaluation of Epidemic Forecasts.pdf;/Users/nikos/github-synced/zotero-nikos/storage/II5XLIM6/2209.html}
}

@article{taylorEvaluatingVolatilityInterval1999,
  title = {Evaluating Volatility and Interval Forecasts},
  author = {Taylor, James W.},
  year = {1999},
  journal = {Journal of Forecasting},
  volume = {18},
  number = {2},
  pages = {111--128},
  issn = {1099-131X},
  doi = {10.1002/(SICI)1099-131X(199903)18:2<111::AID-FOR713>3.0.CO;2-C},
  urldate = {2022-12-07},
  abstract = {A widely used approach to evaluating volatility forecasts uses a regression framework which measures the bias and variance of the forecast. We show that the associated test for bias is inappropriate before introducing a more suitable procedure which is based on the test for bias in a conditional mean forecast. Although volatility has been the most common measure of the variability in a financial time series, in many situations confidence interval forecasts are required. We consider the evaluation of interval forecasts and present a regression-based procedure which uses quantile regression to assess quantile estimator bias and variance. We use exchange rate data to illustrate the proposal by evaluating seven quantile estimators, one of which is a new non-parametric autoregressive conditional heteroscedasticity quantile estimator. The empirical analysis shows that the new evaluation procedure provides useful insight into the quality of quantile estimators. Copyright \textcopyright{} 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {forecast evaluation,interval forecasting,quantile regression,volatility forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/K8NELTS8/(SICI)1099-131X(199903)182111AID-FOR7133.0.html}
}

@article{timmermannForecastingMethodsFinance2018,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  doi = {10.1146/annurev-financial-110217-022713},
  urldate = {2020-12-14},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.}
}

@article{timmermannForecastingMethodsFinance2018a,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  month = nov,
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110217-022713},
  urldate = {2021-12-15},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FHI5V4F/Timmermann - 2018 - Forecasting Methods in Finance.pdf}
}

@article{wallingaHowGenerationIntervals2007,
  title = {How Generation Intervals Shape the Relationship between Growth Rates and Reproductive Numbers},
  author = {Wallinga, J and Lipsitch, M},
  year = {2007},
  month = feb,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {274},
  number = {1609},
  pages = {599--604},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2006.3754},
  urldate = {2022-11-29},
  abstract = {Mathematical models of transmission have become invaluable management tools in planning for the control of emerging infectious diseases. A key variable in such models is the reproductive number R. For new emerging infectious diseases, the value of the reproductive number can only be inferred indirectly from the observed exponential epidemic growth rate r. Such inference is ambiguous as several different equations exist that relate the reproductive number to the growth rate, and it is unclear which of these equations might apply to a new infection. Here, we show that these different equations differ only with respect to their assumed shape of the generation interval distribution. Therefore, the shape of the generation interval distribution determines which equation is appropriate for inferring the reproductive number from the observed growth rate. We show that by assuming all generation intervals to be equal to the mean, we obtain an upper bound to the range of possible values that the reproductive number may attain for a given growth rate. Furthermore, we show that by taking the generation interval distribution equal to the observed distribution, it is possible to obtain an empirical estimate of the reproductive number.},
  keywords = {basic reproduction ratio,epidemiology,influenza,Lotka\textendash Euler equation,serial interval},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LHAEKWXA/Wallinga and Lipsitch - 2007 - How generation intervals shape the relationship be.pdf}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  urldate = {2021-03-04},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}


@Manual{R,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2022},
  url = {https://www.R-project.org/},
}

@Manual{epinow2,
  title = {EpiNow2: Estimate Real-Time Case Counts and Time-Varying Epidemiological Parameters},
  author = {Sam Abbott and Joel Hellewell and Katharine Sherratt and Katelyn Gostic and Joe Hickson and Hamada S. Badr and Michael DeWitt and Robin Thompson and {EpiForecasts} and Sebastian Funk},
  year = {2020},
  doi = {10.5281/zenodo.3957489},
  note = {R package, \url{https://doi.org/10.5281/zenodo.3957490}}
}