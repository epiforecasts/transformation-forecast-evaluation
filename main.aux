\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{apalike}
\citation{heldProbabilisticForecastingInfectious2017}
\citation{reichCollaborativeMultiyearMultimodel2019,funkShorttermForecastsInform2020,cramerEvaluationIndividualEnsemble2021,bracherShorttermForecastingCOVID192021,europeancovid-19forecasthubEuropeanCovid19Forecast2021,sherrattPredictivePerformanceMultimodel2022}
\citation{timmermannForecastingMethodsFinance2018,elliottForecastingEconomicsFinance2016}
\citation{gneitingWeatherForecastingEnsemble2005,kukkonenReviewOperationalRegionalscale2012}
\citation{gneitingStrictlyProperScoring2007}
\citation{gneitingStrictlyProperScoring2007}
\citation{funkAssessingPerformanceRealtime2019}
\citation{bracherEvaluatingEpidemicForecasts2021}
\citation{cramerCOVID19ForecastHub2020,cramerEvaluationIndividualEnsemble2021}
\citation{sherrattPredictivePerformanceMultimodel2022}
\citation{bracherShorttermForecastingCOVID192021,bracherNationalSubnationalShortterm2021}
\citation{CdcepiFlusightforecastdata2022}
\citation{gosticPracticalConsiderationsMeasuring2020}
\citation{dushoffSpeedStrengthEpidemic2021}
\citation{pellisChallengesControlCOVID192021}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{europeancovid-19forecasthubEuropeanCovid19Forecast2021,sherrattPredictivePerformanceMultimodel2022}
\@writefile{toc}{\contentsline {section}{\numberline {2}Transforming forecasts and observations}{3}{section.2}\protected@file@percent }
\newlabel{sec:methods}{{2}{3}{Transforming forecasts and observations}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Interpretation as a score based on multiplicative (relative) errors}{3}{subsection.2.1}\protected@file@percent }
\newlabel{sec:methods:relative}{{2.1}{3}{Interpretation as a score based on multiplicative (relative) errors}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the weighted interval score on the natural and the log scale. The WIS (black line) is shown for a $\mathcal  {N}(1, 0.2)$ predictive distribution (truncated at 0, density represented by the grey area) as a function of the observed value. A: Scores computed on the natural scale (without prior transformation of the predictive distribution and the observed values). The x-axis is shown on a linear scale. C: Scores computed on the logarithmic scale (log-transforming the predictive distribution and the observed values prior to scoring). The x-axis is shown on a linear scale. B, D: Like A, C, but the x-axis is shown on a relative scale.}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:change-in-scores}{{1}{4}{Illustration of the weighted interval score on the natural and the log scale. The WIS (black line) is shown for a $\mathcal {N}(1, 0.2)$ predictive distribution (truncated at 0, density represented by the grey area) as a function of the observed value. A: Scores computed on the natural scale (without prior transformation of the predictive distribution and the observed values). The x-axis is shown on a linear scale. C: Scores computed on the logarithmic scale (log-transforming the predictive distribution and the observed values prior to scoring). The x-axis is shown on a linear scale. B, D: Like A, C, but the x-axis is shown on a relative scale}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Interpretation as an approximate score of the growth rate}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:methods:growthrate}{{2.2}{4}{Interpretation as an approximate score of the growth rate}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Effects on model rankings}{5}{subsection.2.3}\protected@file@percent }
\newlabel{sec:methods:rankings}{{2.3}{5}{Effects on model rankings}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Simulation showing how the relationship between mean and variance of the forecast quantity affects scores. 1,000 samples were drawn from different negative binomial distributions and scores were computed assuming an ideal forecaster (forecast distribution equal to data-generating distribution). Observations were simulated for different means of the negative-binomial distributions (representing, for example, the number of cases in differently sized states) as well as different relationships between mean and variance (representing, for example, three different infectious processes). The variance of the negative binomial is given as $\sigma ^2 = \mu + \mu ^2 / \theta $, meaning that for large theta the negative binomial distribution is equal to the Poisson distribution. We used values of $\theta = 0.1$ (red), 1 (green) and 1b (blue). To make the scores for the different distributions comparable, scores were normalised to one, meaning that the mean score for every distribution (red, green, blue) is one. A: Normalised WIS for ideal forecasts with increasing means of three distribution with different relationships between mean and variance. B: A but with log scale axis.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:SIM-wis-state-size-mean}{{2}{6}{Simulation showing how the relationship between mean and variance of the forecast quantity affects scores. 1,000 samples were drawn from different negative binomial distributions and scores were computed assuming an ideal forecaster (forecast distribution equal to data-generating distribution). Observations were simulated for different means of the negative-binomial distributions (representing, for example, the number of cases in differently sized states) as well as different relationships between mean and variance (representing, for example, three different infectious processes). The variance of the negative binomial is given as $\sigma ^2 = \mu + \mu ^2 / \theta $, meaning that for large theta the negative binomial distribution is equal to the Poisson distribution. We used values of $\theta = 0.1$ (red), 1 (green) and 1b (blue). To make the scores for the different distributions comparable, scores were normalised to one, meaning that the mean score for every distribution (red, green, blue) is one. A: Normalised WIS for ideal forecasts with increasing means of three distribution with different relationships between mean and variance. B: A but with log scale axis}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Practical considerations}{6}{subsection.2.4}\protected@file@percent }
\newlabel{sec:methods:considerations}{{2.4}{6}{Practical considerations}{subsection.2.4}{}}
\citation{europeancovid-19forecasthubEuropeanCovid19Forecast2021,sherrattPredictivePerformanceMultimodel2022}
\citation{cramerEvaluationIndividualEnsemble2021}
\citation{bracherShorttermForecastingCOVID192021}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Scores for different forecasts evaluated using the WIS, the WIS on the log-scale and the logarithm of the WIS. We simulated 1000 observations $Y_i = {\rm  e}^{x_i}$, with $x_i \text  { iid} \sim \mathcal  {N}(0, 1)$. We then simulated 20 forecasters who would issue a predictive distribution $F = {\rm  e}^{x_i}$, with $x \sim \mathcal  {N}(0, \sigma )$, with values of $\sigma $ ranging from 0.1 to 2. For forecasts evaluated by the WIS and the WIS on log-transformed values, the score is minimised when the predictive distribution equals the data-generating distribution. When taking the logarithm of the WIS, scores are minimised for a predictive distribution that is too narrow compared to the data-generating distribution.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:log-improper}{{3}{7}{Scores for different forecasts evaluated using the WIS, the WIS on the log-scale and the logarithm of the WIS. We simulated 1000 observations $Y_i = {\rm e}^{x_i}$, with $x_i \text { iid} \sim \mathcal {N}(0, 1)$. We then simulated 20 forecasters who would issue a predictive distribution $F = {\rm e}^{x_i}$, with $x \sim \mathcal {N}(0, \sigma )$, with values of $\sigma $ ranging from 0.1 to 2. For forecasts evaluated by the WIS and the WIS on log-transformed values, the score is minimised when the predictive distribution equals the data-generating distribution. When taking the logarithm of the WIS, scores are minimised for a predictive distribution that is too narrow compared to the data-generating distribution}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Empirical example: the European Forecast Hub}{7}{section.3}\protected@file@percent }
\newlabel{sec:results}{{3}{7}{Empirical example: the European Forecast Hub}{section.3}{}}
\citation{cramerEvaluationIndividualEnsemble2021}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Observations and scores across locations and forecast horizons for the European COVID-19 Forecast Hub data. Locations are sorted according to the mean observed value in that location. A: Average (across all time points) of observed cases and deaths for different locations. B: Corresponding boxplot (y-axis on log-scale) of all cases and deaths. C: Scores for two-week-ahead forecasts from the EuroCOVIDhub-ensemble (averaged across all forecast dates) for different locations, evaluated on the natural as well as the logarithmic scale. D: Corresponding boxplots of all individual scores for two-week-ahead predictions. E: Boxplots for the relative change of scores for the EuroCOVIDhub-ensemble across forecast horizons. For any given forecast date and location, forecasts were made for four different forecast horizons, resulting in four scores. All scores were divided by the score for forecast horizon one.}}{9}{figure.4}\protected@file@percent }
\newlabel{fig:HUB-mean-locations}{{4}{9}{Observations and scores across locations and forecast horizons for the European COVID-19 Forecast Hub data. Locations are sorted according to the mean observed value in that location. A: Average (across all time points) of observed cases and deaths for different locations. B: Corresponding boxplot (y-axis on log-scale) of all cases and deaths. C: Scores for two-week-ahead forecasts from the EuroCOVIDhub-ensemble (averaged across all forecast dates) for different locations, evaluated on the natural as well as the logarithmic scale. D: Corresponding boxplots of all individual scores for two-week-ahead predictions. E: Boxplots for the relative change of scores for the EuroCOVIDhub-ensemble across forecast horizons. For any given forecast date and location, forecasts were made for four different forecast horizons, resulting in four scores. All scores were divided by the score for forecast horizon one}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Correlations between observations and scores. A: Mean WIS for two-week-ahead predictions of the EuroCOVIDhub-ensemble against the average number of observations in a location. B: Average Spearman rank correlation of scores for individual forecasts from all models. For every individual target (defined by a combination of forecast date, target type, horizon, location), one score was obtained per model. Then, the rank correlation was computed between the scores for all models on the natural scale vs. on the log scale. All rank correlations were averaged and stratified by horizon and target type. C: Correlation between relative skill scores. For every forecast horizon and target type, a separate rel. skill score was computed per model using pairwise comparisons. The plot shows the correlation between the rel. skill scores on the natural vs. on the log scale.}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:HUB-cors}{{5}{10}{Correlations between observations and scores. A: Mean WIS for two-week-ahead predictions of the EuroCOVIDhub-ensemble against the average number of observations in a location. B: Average Spearman rank correlation of scores for individual forecasts from all models. For every individual target (defined by a combination of forecast date, target type, horizon, location), one score was obtained per model. Then, the rank correlation was computed between the scores for all models on the natural scale vs. on the log scale. All rank correlations were averaged and stratified by horizon and target type. C: Correlation between relative skill scores. For every forecast horizon and target type, a separate rel. skill score was computed per model using pairwise comparisons. The plot shows the correlation between the rel. skill scores on the natural vs. on the log scale}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Forecasts and scores for two-week-ahead predictions from the EuroCOVIDhub-ensemble made in Germany. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores. }}{11}{figure.6}\protected@file@percent }
\newlabel{fig:HUB-model-comparison-ensemble}{{6}{11}{Forecasts and scores for two-week-ahead predictions from the EuroCOVIDhub-ensemble made in Germany. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{11}{section.4}\protected@file@percent }
\newlabel{sec:discussion}{{4}{11}{Discussion}{section.4}{}}
\citation{bracherEvaluatingEpidemicForecasts2021}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Changes in model ratings as measured by relative skill for two-week-ahead predictions for cases (top row) and deaths (bottom row). A: Relative skill scores for case forecasts from different models submitted to the European COVID-19 Forecast Hub computed on the natural scale. B: Change in rankings as determined by relative skill scores when moving from an evaluation on the natural scale to one on the logarithmic scale. C: Relative skill scores based on scores on the log scale. D: Difference in relative skill scores computed on the natural and on the logarithmic scale. E: Relative contributions of the different WIS components (overprediction, underprediction, and dispersion) to overall model scores on the natural and the logarithmic scale. F, G, H, I, J, K: Analogously for deaths.}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:HUB-rank-order}{{7}{12}{Changes in model ratings as measured by relative skill for two-week-ahead predictions for cases (top row) and deaths (bottom row). A: Relative skill scores for case forecasts from different models submitted to the European COVID-19 Forecast Hub computed on the natural scale. B: Change in rankings as determined by relative skill scores when moving from an evaluation on the natural scale to one on the logarithmic scale. C: Relative skill scores based on scores on the log scale. D: Difference in relative skill scores computed on the natural and on the logarithmic scale. E: Relative contributions of the different WIS components (overprediction, underprediction, and dispersion) to overall model scores on the natural and the logarithmic scale. F, G, H, I, J, K: Analogously for deaths}{figure.7}{}}
\citation{srivastavaShapebasedEvaluationEpidemic2022}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary information}{14}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary statistics for observations and scores for forecasts from the ECDC data set.}}{14}{table.1}\protected@file@percent }
\newlabel{tab:HUB-summary}{{1}{14}{Summary statistics for observations and scores for forecasts from the ECDC data set}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Additional information on the WIS}{14}{subsection.A.1}\protected@file@percent }
\newlabel{wis}{{A.1}{14}{Additional information on the WIS}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Additional information on the CRPS}{14}{subsection.A.2}\protected@file@percent }
\newlabel{crps}{{A.2}{14}{Additional information on the CRPS}{subsection.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Additional figures}{15}{subsection.A.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Number of forecasts available from different models for each forecast date. }}{15}{figure.8}\protected@file@percent }
\newlabel{fig:HUB-num-avail-models}{{8}{15}{Number of forecasts available from different models for each forecast date}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Forecasts and scores for two-week-ahead predictions from the EuroCOVIDhub-baseline made in Germany. The model had zero included in some of its 50 percent intervals (e.g. for case forecasts in July), leading to excessive dispersion values on the log scale. One could argue that including zero in the prediction intervals constituted an unreasonable forecast that was rightly penalised, but in general care has to be taken with small numbers. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores. }}{16}{figure.9}\protected@file@percent }
\newlabel{fig:HUB-model-comparison-baseline}{{9}{16}{Forecasts and scores for two-week-ahead predictions from the EuroCOVIDhub-baseline made in Germany. The model had zero included in some of its 50 percent intervals (e.g. for case forecasts in July), leading to excessive dispersion values on the log scale. One could argue that including zero in the prediction intervals constituted an unreasonable forecast that was rightly penalised, but in general care has to be taken with small numbers. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Forecasts and scores for two-week-ahead predictions from the epiforecasts-EpiNow2 model made in Germany. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores. }}{17}{figure.10}\protected@file@percent }
\newlabel{fig:HUB-model-comparison-epinow}{{10}{17}{Forecasts and scores for two-week-ahead predictions from the epiforecasts-EpiNow2 model made in Germany. A, E: 50\% and 90\% prediction intervals and observed values for cases and deaths on the natural scale. B, F: Corresponding scores. C, G: Forecasts and observations on the log scale. D, H: Corresponding scores}{figure.10}{}}
\bibdata{log-or-not.bib}
\bibcite{CdcepiFlusightforecastdata2022}{{1}{2022}{{Cdc}}{{}}}
\bibcite{bracherEvaluatingEpidemicForecasts2021}{{2}{2021a}{{Bracher et~al.}}{{}}}
\bibcite{bracherShorttermForecastingCOVID192021}{{3}{2021b}{{Bracher et~al.}}{{}}}
\bibcite{bracherNationalSubnationalShortterm2021}{{4}{2021c}{{Bracher et~al.}}{{}}}
\bibcite{cramerEvaluationIndividualEnsemble2021}{{5}{2021}{{Cramer et~al.}}{{}}}
\bibcite{cramerCOVID19ForecastHub2020}{{6}{2020}{{Cramer et~al.}}{{}}}
\bibcite{dushoffSpeedStrengthEpidemic2021}{{7}{2021}{{Dushoff and Park}}{{}}}
\bibcite{elliottForecastingEconomicsFinance2016}{{8}{2016}{{Elliott and Timmermann}}{{}}}
\bibcite{europeancovid-19forecasthubEuropeanCovid19Forecast2021}{{9}{2021}{{European Covid-19 Forecast Hub}}{{}}}
\bibcite{funkShorttermForecastsInform2020}{{10}{2020}{{Funk et~al.}}{{}}}
\bibcite{funkAssessingPerformanceRealtime2019}{{11}{2019}{{Funk et~al.}}{{}}}
\bibcite{gneitingWeatherForecastingEnsemble2005}{{12}{2005}{{Gneiting and Raftery}}{{}}}
\bibcite{gneitingStrictlyProperScoring2007}{{13}{2007}{{Gneiting and Raftery}}{{}}}
\bibcite{gosticPracticalConsiderationsMeasuring2020}{{14}{2020}{{Gostic et~al.}}{{}}}
\bibcite{heldProbabilisticForecastingInfectious2017}{{15}{2017}{{Held et~al.}}{{}}}
\bibcite{kukkonenReviewOperationalRegionalscale2012}{{16}{2012}{{Kukkonen et~al.}}{{}}}
\bibcite{pellisChallengesControlCOVID192021}{{17}{2021}{{Pellis et~al.}}{{}}}
\bibcite{reichCollaborativeMultiyearMultimodel2019}{{18}{2019}{{Reich et~al.}}{{}}}
\bibcite{sherrattPredictivePerformanceMultimodel2022}{{19}{2022}{{Sherratt et~al.}}{{}}}
\bibcite{srivastavaShapebasedEvaluationEpidemic2022}{{20}{2022}{{Srivastava et~al.}}{{}}}
\bibcite{timmermannForecastingMethodsFinance2018}{{21}{2018}{{Timmermann}}{{}}}
\gdef \@abspage@last{20}
